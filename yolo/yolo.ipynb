{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "!pip install imgaug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcsfuse images_bucket ~/data/images\n",
    "!gcsfuse other_bucket ~/data/other\n",
    "#!gcsfuse model_bucket ~/model/variables\n",
    "#!gcsfuse sn_test_bucket ~/sn_test_data\n",
    "#!gcsfuse sn_train_bucket ~/data/sn_images\n",
    "#!gcsfuse test_bucket ~/test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow as base library for neural networks\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras as a layer on top of tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# h5py is needed to store and load Keras models\n",
    "import h5py\n",
    " \n",
    "# matplotlib is needed to plot bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os, cv2, time, random\n",
    "import ntpath\n",
    "\n",
    "import utils\n",
    "\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "from utils import BoundBox, normalize, bbox_iou, decode_netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = Path(\"data\")\n",
    "image_folder = working_folder / \"images\"\n",
    "\n",
    "hamamatsu_rx_ids = list(range(0, 51))\n",
    "hamamatsu_360_ids = list(range(51, 101))\n",
    "aperio_ids = list(range(101, 151))\n",
    "leica_ids = list(range(151, 201))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>box</th>\n",
       "      <th>cat</th>\n",
       "      <th>scanner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>150.tiff</td>\n",
       "      <td>150</td>\n",
       "      <td>6467</td>\n",
       "      <td>4862</td>\n",
       "      <td>[569, 4276, 619, 4326]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Aperio CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>150.tiff</td>\n",
       "      <td>150</td>\n",
       "      <td>6467</td>\n",
       "      <td>4862</td>\n",
       "      <td>[3730, 4538, 3780, 4588]</td>\n",
       "      <td>mitotic figure</td>\n",
       "      <td>Aperio CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>150.tiff</td>\n",
       "      <td>150</td>\n",
       "      <td>6467</td>\n",
       "      <td>4862</td>\n",
       "      <td>[4318, 4138, 4368, 4188]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Aperio CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>150.tiff</td>\n",
       "      <td>150</td>\n",
       "      <td>6467</td>\n",
       "      <td>4862</td>\n",
       "      <td>[5643, 1318, 5693, 1368]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Aperio CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>150.tiff</td>\n",
       "      <td>150</td>\n",
       "      <td>6467</td>\n",
       "      <td>4862</td>\n",
       "      <td>[2681.5, 2846, 2731.5, 2896]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Aperio CS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name  image_id  width  height                           box  \\\n",
       "4430  150.tiff       150   6467    4862        [569, 4276, 619, 4326]   \n",
       "4431  150.tiff       150   6467    4862      [3730, 4538, 3780, 4588]   \n",
       "4432  150.tiff       150   6467    4862      [4318, 4138, 4368, 4188]   \n",
       "4433  150.tiff       150   6467    4862      [5643, 1318, 5693, 1368]   \n",
       "4434  150.tiff       150   6467    4862  [2681.5, 2846, 2731.5, 2896]   \n",
       "\n",
       "                 cat    scanner  \n",
       "4430   hard negative  Aperio CS  \n",
       "4431  mitotic figure  Aperio CS  \n",
       "4432   hard negative  Aperio CS  \n",
       "4433   hard negative  Aperio CS  \n",
       "4434   hard negative  Aperio CS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_file = working_folder / \"other\" / \"MIDOG.json\"\n",
    "rows = []\n",
    "with open(annotation_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    categories = {1: 'mitotic figure', 2: 'hard negative'}\n",
    "\n",
    "    for row in data[\"images\"]:\n",
    "        file_name = row[\"file_name\"]\n",
    "        image_id = row[\"id\"]\n",
    "        width = row[\"width\"]\n",
    "        height = row[\"height\"]\n",
    "\n",
    "        scanner  = \"Hamamatsu XR\"\n",
    "        if image_id in hamamatsu_360_ids:\n",
    "            scanner  = \"Hamamatsu S360\"\n",
    "        if image_id in aperio_ids:\n",
    "            scanner  = \"Aperio CS\"\n",
    "        if image_id in leica_ids:\n",
    "            scanner  = \"Leica GT450\"\n",
    "\n",
    "        for annotation in [anno for anno in data['annotations'] if anno[\"image_id\"] == image_id]:\n",
    "            box = annotation[\"bbox\"]\n",
    "            cat = categories[annotation[\"category_id\"]]\n",
    "\n",
    "            rows.append([file_name, image_id, width, height, box, cat, scanner])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"file_name\", \"image_id\", \"width\", \"height\", \"box\", \"cat\", \"scanner\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a dictionary with all boxes for each image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4336, 346, 4386, 396, 'hard negative'],\n",
       " [756, 872, 806, 922, 'hard negative'],\n",
       " [270, 4044, 320, 4094, 'hard negative'],\n",
       " [6672.5, 706.5, 6722.5, 756.5, 'hard negative']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['box_with_cat'] = df.apply(lambda row: [row['box'][0], row['box'][1], row['box'][2], row['box'][3], row['cat']], axis = 1)\n",
    "grouped_boxes_df = df[['image_id', 'box_with_cat']].groupby('image_id').box_with_cat.apply(list).reset_index()\n",
    "box_dict = grouped_boxes_df.set_index('image_id').T.to_dict('list')\n",
    "box_dict.get(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_slice_data(width, height, box, box_size, slice_size):\n",
    "    \n",
    "    (b_xmin, b_ymin, b_xmax, b_ymax) = box\n",
    "    \n",
    "    if b_xmin < 0: b_xmin = 0\n",
    "    if b_ymin < 0: b_ymin = 0\n",
    "    if b_xmax > width: b_xmax = width\n",
    "    if b_ymax > height: b_ymax = height\n",
    "    \n",
    "    xmin = random.randint(max(0, int(math.ceil(b_xmax) - slice_size)), min(int(b_xmin), width - slice_size)) \n",
    "    ymin = random.randint(max(0, int(math.ceil(b_ymax) - slice_size)), min(int(b_ymin), height - slice_size)) \n",
    "\n",
    "    return (xmin, ymin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes_for_slice(slice_xy, slice_size, image_boxes, box_size):\n",
    "    boxes = []\n",
    "    for box in image_boxes:\n",
    "        (b_xmin, b_ymin, _, _, cat) = box\n",
    "        if ((box[0] > slice_xy[0] - box_size/2 and box[0] < slice_xy[0] + slice_size - box_size/2) and\n",
    "            (box[1] > slice_xy[1] - box_size/2 and box[1] < slice_xy[1] + slice_size - box_size/2)):\n",
    "                boxes.append(box)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a random image slice around every entity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>box</th>\n",
       "      <th>cat</th>\n",
       "      <th>scanner</th>\n",
       "      <th>box_with_cat</th>\n",
       "      <th>slice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[4336, 346, 4386, 396]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[4336, 346, 4386, 396, hard negative]</td>\n",
       "      <td>(4233, 317)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[756, 872, 806, 922]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[756, 872, 806, 922, hard negative]</td>\n",
       "      <td>(577, 831)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[270, 4044, 320, 4094]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[270, 4044, 320, 4094, hard negative]</td>\n",
       "      <td>(129, 3865)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[6672.5, 706.5, 6722.5, 756.5]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[6672.5, 706.5, 6722.5, 756.5, hard negative]</td>\n",
       "      <td>(6478, 660)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002.tiff</td>\n",
       "      <td>2</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[1872, 319, 1922, 369]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[1872, 319, 1922, 369, hard negative]</td>\n",
       "      <td>(1826, 210)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  image_id  width  height                             box  \\\n",
       "0  001.tiff         1   7215    5412          [4336, 346, 4386, 396]   \n",
       "1  001.tiff         1   7215    5412            [756, 872, 806, 922]   \n",
       "2  001.tiff         1   7215    5412          [270, 4044, 320, 4094]   \n",
       "3  001.tiff         1   7215    5412  [6672.5, 706.5, 6722.5, 756.5]   \n",
       "4  002.tiff         2   7215    5412          [1872, 319, 1922, 369]   \n",
       "\n",
       "             cat       scanner                                   box_with_cat  \\\n",
       "0  hard negative  Hamamatsu XR          [4336, 346, 4386, 396, hard negative]   \n",
       "1  hard negative  Hamamatsu XR            [756, 872, 806, 922, hard negative]   \n",
       "2  hard negative  Hamamatsu XR          [270, 4044, 320, 4094, hard negative]   \n",
       "3  hard negative  Hamamatsu XR  [6672.5, 706.5, 6722.5, 756.5, hard negative]   \n",
       "4  hard negative  Hamamatsu XR          [1872, 319, 1922, 369, hard negative]   \n",
       "\n",
       "         slice  \n",
       "0  (4233, 317)  \n",
       "1   (577, 831)  \n",
       "2  (129, 3865)  \n",
       "3  (6478, 660)  \n",
       "4  (1826, 210)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_size = 50\n",
    "slice_size = 256\n",
    "\n",
    "df[\"slice\"] = df.apply(lambda row: get_random_slice_data(\n",
    "    row.width,\n",
    "    row.height,\n",
    "    row.box,\n",
    "    box_size,\n",
    "    slice_size\n",
    "), axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get other boxes that also fit into a random slice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>box</th>\n",
       "      <th>cat</th>\n",
       "      <th>scanner</th>\n",
       "      <th>box_with_cat</th>\n",
       "      <th>slice</th>\n",
       "      <th>slice_boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[4336, 346, 4386, 396]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[4336, 346, 4386, 396, hard negative]</td>\n",
       "      <td>(4233, 317)</td>\n",
       "      <td>[[4336, 346, 4386, 396, hard negative]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[756, 872, 806, 922]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[756, 872, 806, 922, hard negative]</td>\n",
       "      <td>(577, 831)</td>\n",
       "      <td>[[756, 872, 806, 922, hard negative]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[270, 4044, 320, 4094]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[270, 4044, 320, 4094, hard negative]</td>\n",
       "      <td>(129, 3865)</td>\n",
       "      <td>[[270, 4044, 320, 4094, hard negative]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001.tiff</td>\n",
       "      <td>1</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[6672.5, 706.5, 6722.5, 756.5]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[6672.5, 706.5, 6722.5, 756.5, hard negative]</td>\n",
       "      <td>(6478, 660)</td>\n",
       "      <td>[[6672.5, 706.5, 6722.5, 756.5, hard negative]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002.tiff</td>\n",
       "      <td>2</td>\n",
       "      <td>7215</td>\n",
       "      <td>5412</td>\n",
       "      <td>[1872, 319, 1922, 369]</td>\n",
       "      <td>hard negative</td>\n",
       "      <td>Hamamatsu XR</td>\n",
       "      <td>[1872, 319, 1922, 369, hard negative]</td>\n",
       "      <td>(1826, 210)</td>\n",
       "      <td>[[1872, 319, 1922, 369, hard negative]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  image_id  width  height                             box  \\\n",
       "0  001.tiff         1   7215    5412          [4336, 346, 4386, 396]   \n",
       "1  001.tiff         1   7215    5412            [756, 872, 806, 922]   \n",
       "2  001.tiff         1   7215    5412          [270, 4044, 320, 4094]   \n",
       "3  001.tiff         1   7215    5412  [6672.5, 706.5, 6722.5, 756.5]   \n",
       "4  002.tiff         2   7215    5412          [1872, 319, 1922, 369]   \n",
       "\n",
       "             cat       scanner                                   box_with_cat  \\\n",
       "0  hard negative  Hamamatsu XR          [4336, 346, 4386, 396, hard negative]   \n",
       "1  hard negative  Hamamatsu XR            [756, 872, 806, 922, hard negative]   \n",
       "2  hard negative  Hamamatsu XR          [270, 4044, 320, 4094, hard negative]   \n",
       "3  hard negative  Hamamatsu XR  [6672.5, 706.5, 6722.5, 756.5, hard negative]   \n",
       "4  hard negative  Hamamatsu XR          [1872, 319, 1922, 369, hard negative]   \n",
       "\n",
       "         slice                                      slice_boxes  \n",
       "0  (4233, 317)          [[4336, 346, 4386, 396, hard negative]]  \n",
       "1   (577, 831)            [[756, 872, 806, 922, hard negative]]  \n",
       "2  (129, 3865)          [[270, 4044, 320, 4094, hard negative]]  \n",
       "3  (6478, 660)  [[6672.5, 706.5, 6722.5, 756.5, hard negative]]  \n",
       "4  (1826, 210)          [[1872, 319, 1922, 369, hard negative]]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"slice_boxes\"] = df.apply(lambda row: get_boxes_for_slice(\n",
    "    row.slice,\n",
    "    slice_size,\n",
    "    box_dict.get(row.image_id)[0],\n",
    "    box_size,\n",
    "), axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save each slice to disk, so the model doesn't need to open every time the whole image for each slice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(df, img_dir, train_dir, slice_size):\n",
    "    all_imgs = []\n",
    "    seen_labels = {}\n",
    "    \n",
    "    for i in range(1, 151):\n",
    "        if i in [30, 60, 90, 120]:\n",
    "            print(i, \"finished!\")\n",
    "        \n",
    "        temp_df = df[df['image_id'] == i]\n",
    "        #temp_name = 'output' + str(i) + '.tiff'\n",
    "        temp_name = temp_df.iloc[0][0]\n",
    "         \n",
    "        temp_image = cv2.imread(img_dir + temp_name)\n",
    "    \n",
    "        for index, row in temp_df.iterrows():\n",
    "\n",
    "            img = {'object':[]}\n",
    "\n",
    "            _, _, _, _, _, _, _, _, im_slice, slice_boxes = row\n",
    "\n",
    "            image = temp_image[im_slice[1]:im_slice[1]+slice_size, im_slice[0]:im_slice[0]+slice_size]\n",
    "            train_name = str(index) + '.tiff'\n",
    "            cv2.imwrite(train_dir + train_name, image)\n",
    "\n",
    "            img['file_name'] = train_dir + train_name\n",
    "            img['width'] = slice_size\n",
    "            img['height'] = slice_size\n",
    "\n",
    "            for box in slice_boxes:\n",
    "                (b_xmin, b_ymin, b_xmax, b_ymax, cat) = box\n",
    "                obj = {}\n",
    "                obj['name'] = cat\n",
    "                img['object'] += [obj]\n",
    "\n",
    "                obj['xmin'] = b_xmin - im_slice[0]\n",
    "                obj['ymin'] = b_ymin - im_slice[1]\n",
    "                obj['xmax'] = b_xmax - im_slice[0]\n",
    "                obj['ymax'] = b_ymax - im_slice[1]\n",
    "\n",
    "            if len(img['object']) > 0:\n",
    "                all_imgs += [img]\n",
    "        \n",
    "    return all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 finished!\n",
      "60 finished!\n",
      "90 finished!\n",
      "120 finished!\n"
     ]
    }
   ],
   "source": [
    "image_path = os.path.join('./', 'data', 'images/')\n",
    "train_path = os.path.join('./', 'data', 'train/')\n",
    "all_imgs = parse_annotation(df, image_path, train_path, slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['hard negative', 'mitotic figure']\n",
    "\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "\n",
    "IMAGE_H, IMAGE_W = slice_size, slice_size\n",
    "GRID_H,  GRID_W  = 16 , 16\n",
    "BOX              = 5\n",
    "OBJ_THRESHOLD    = 0.3\n",
    "NMS_THRESHOLD    = 0.1\n",
    "ANCHORS          = [2.2, 2.2, 2.7, 2.7, 3.2, 3.2, 3.7, 3.7, 4.2, 4.2]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 32\n",
    "WARM_UP_BATCHES  = 100\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'BOX'             : BOX,\n",
    "    'LABELS'          : LABELS,\n",
    "    'CLASS'           : len(LABELS),\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_to_depth_x2(x):\n",
    "    return tf.nn.space_to_depth(x, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLO_network(input_img,true_bxs,CLASS):\n",
    "\n",
    "    # Layer 1\n",
    "    x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_img)\n",
    "    x = BatchNormalization(name='norm_1')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 2\n",
    "    x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_2')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 3\n",
    "    x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_3')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 4\n",
    "    x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_4')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 5\n",
    "    x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_5')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Layer 6\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_6')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 7\n",
    "    x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_7')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 8\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False, input_shape=(512,512,3))(x)\n",
    "    x = BatchNormalization(name='norm_8')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 9\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_9')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 10\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_10')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 11\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_11')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 12\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_12')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 13\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_13')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    skip_connection = x\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 14\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_14')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 15\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_15')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 16\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_16')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 17\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_17')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 18\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_18')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 19\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_19')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 20\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_20')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 21\n",
    "    skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "    skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "    skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "    skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
    "\n",
    "    x = concatenate([skip_connection, x])\n",
    "\n",
    "    # Layer 22\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_22')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 23\n",
    "    x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "    output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "    output = Lambda(lambda args: args[0])([output, true_bxs])\n",
    "\n",
    "    model = Model([input_img, true_bxs], output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 256, 256, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_1 (BatchNormalization)     (None, 256, 256, 32) 128         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 256, 256, 32) 0           norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 128, 128, 64) 18432       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_2 (BatchNormalization)     (None, 128, 128, 64) 256         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 64) 0           norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 64, 64, 128)  73728       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_3 (BatchNormalization)     (None, 64, 64, 128)  512         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 128)  0           norm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 64, 64, 64)   8192        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_4 (BatchNormalization)     (None, 64, 64, 64)   256         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 64)   0           norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 64, 64, 128)  73728       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_5 (BatchNormalization)     (None, 64, 64, 128)  512         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 64, 64, 128)  0           norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 32, 32, 256)  294912      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_6 (BatchNormalization)     (None, 32, 32, 256)  1024        conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 256)  0           norm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 32, 32, 128)  32768       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_7 (BatchNormalization)     (None, 32, 32, 128)  512         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 128)  0           norm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 32, 32, 256)  294912      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_8 (BatchNormalization)     (None, 32, 32, 256)  1024        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 256)  0           norm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 32, 32, 512)  1179648     leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_9 (BatchNormalization)     (None, 32, 32, 512)  2048        conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 32, 32, 512)  0           norm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 32, 32, 256)  131072      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_10 (BatchNormalization)    (None, 32, 32, 256)  1024        conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 32, 32, 256)  0           norm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, 32, 32, 512)  1179648     leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_11 (BatchNormalization)    (None, 32, 32, 512)  2048        conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 32, 32, 512)  0           norm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 32, 32, 256)  131072      leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_12 (BatchNormalization)    (None, 32, 32, 256)  1024        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 32, 32, 256)  0           norm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 32, 32, 512)  1179648     leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_13 (BatchNormalization)    (None, 32, 32, 512)  2048        conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 32, 32, 512)  0           norm_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 16, 16, 1024) 4718592     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "norm_14 (BatchNormalization)    (None, 16, 16, 1024) 4096        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 16, 16, 1024) 0           norm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 16, 16, 512)  524288      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_15 (BatchNormalization)    (None, 16, 16, 512)  2048        conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 16, 16, 512)  0           norm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 16, 16, 1024) 4718592     leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_16 (BatchNormalization)    (None, 16, 16, 1024) 4096        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 16, 16, 1024) 0           norm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 16, 16, 512)  524288      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_17 (BatchNormalization)    (None, 16, 16, 512)  2048        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 16, 16, 512)  0           norm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 16, 16, 1024) 4718592     leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_18 (BatchNormalization)    (None, 16, 16, 1024) 4096        conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 16, 16, 1024) 0           norm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 16, 16, 1024) 9437184     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "norm_19 (BatchNormalization)    (None, 16, 16, 1024) 4096        conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 32, 32, 64)   32768       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 16, 16, 1024) 0           norm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_21 (BatchNormalization)    (None, 32, 32, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 16, 16, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 32, 32, 64)   0           norm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_20 (BatchNormalization)    (None, 16, 16, 1024) 4096        conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16, 16, 256)  0           leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 16, 16, 1024) 0           norm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 1280) 0           lambda[0][0]                     \n",
      "                                                                 leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 16, 16, 1024) 11796480    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "norm_22 (BatchNormalization)    (None, 16, 16, 1024) 4096        conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 16, 16, 1024) 0           norm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 16, 16, 35)   35875       leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 16, 16, 5, 7) 0           conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1, 1, 1, 50, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16, 16, 5, 7) 0           reshape[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 50,583,811\n",
      "Trainable params: 50,563,139\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "model = YOLO_network(input_image, true_boxes, CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bounding_box(image, model, obj_threshold, nms_threshold, anchors, nb_class):\n",
    "    \"\"\"\n",
    "        Predict bounding boxes for a given image.\n",
    "    \"\"\"    \n",
    "    input_image = image / 255. # rescale intensity to [0, 1]\n",
    "    input_image = input_image[:,:,::-1]\n",
    "    img_shape = image.shape\n",
    "    input_image = np.expand_dims(input_image, 0) \n",
    "\n",
    "    # define variable needed to process input image\n",
    "    dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "    # get output from network\n",
    "    netout = model.predict([input_image, dummy_array])\n",
    "    \n",
    "    return utils.decode_netout(netout[0], obj_threshold, nms_threshold, anchors, nb_class)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from loss_utils import *\n",
    "\n",
    "class Loss(tf.keras.losses.Loss):\n",
    "\n",
    "    EPSILON = 1e-6\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        grid_width,\n",
    "        grid_height,\n",
    "        anchors,\n",
    "        n_boxes=1000,\n",
    "        scales=Scales()\n",
    "    ):\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._grid_width = grid_width\n",
    "        self._grid_height = grid_height\n",
    "\n",
    "        self._n_boxes = n_boxes\n",
    "        self._anchors = anchors\n",
    "        self._scales = scales\n",
    "\n",
    "        self._grid = get_cell_grid(\n",
    "            self._batch_size, self._n_boxes, self._grid_width, self._grid_height\n",
    "        )\n",
    "        \n",
    "        self.reduction = tf.keras.losses.Reduction.AUTO\n",
    "        self.name = 'loss'\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "\n",
    "        # prediction\n",
    "        pred_box_xy = tf.sigmoid(y_pred[..., :2]) + self._grid\n",
    "        pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(\n",
    "            self._anchors, [1, 1, 1, self._n_boxes, 2]\n",
    "        )\n",
    "        pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "        pred_box_class = y_pred[..., 5:]\n",
    "\n",
    "        # ground thuth\n",
    "        true_box_xy = y_true[..., :2]\n",
    "        true_box_wh = y_true[..., 2:4]\n",
    "        true_box_conf = (\n",
    "            tf_iou(true_box_xy, true_box_wh, pred_box_xy, pred_box_wh) * y_true[..., 4]\n",
    "        )\n",
    "        true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "\n",
    "        # masks\n",
    "        # TODO calculate responsible anchorbox (not neccesary when one anchor box)\n",
    "\n",
    "        # expand for  2 values (x,y) and (w,h) for coord scale\n",
    "        coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * self._scales.coord_scale\n",
    "        conf_mask = (y_true[..., 4] + self._scales.no_object_scale) * self._scales.object_scale\n",
    "        class_mask = y_true[..., 4] * self._scales.class_scale\n",
    "\n",
    "        # normalization factor\n",
    "        nb_coord_norm = tf.reduce_sum(tf.cast(coord_mask > 0.0, dtype=tf.float32)) + Loss.EPSILON\n",
    "        nb_conf_norm = tf.reduce_sum(tf.cast(conf_mask > 0.0, dtype=tf.float32)) + Loss.EPSILON\n",
    "        nb_class_norm = tf.reduce_sum(tf.cast(class_mask > 0.0, dtype=tf.float32)) + Loss.EPSILON\n",
    "\n",
    "        # losses\n",
    "        loss_xy = (\n",
    "            tf.reduce_sum(tf.square(true_box_xy - pred_box_xy) * coord_mask)\n",
    "            / nb_coord_norm\n",
    "            / 2.0\n",
    "        )\n",
    "        loss_wh = (\n",
    "            tf.reduce_sum(tf.square(true_box_wh - pred_box_wh) * coord_mask)\n",
    "            / nb_coord_norm\n",
    "            / 2.0\n",
    "        )\n",
    "        loss_conf = (\n",
    "            tf.reduce_sum(tf.square(true_box_conf - pred_box_conf) * conf_mask)\n",
    "            / nb_conf_norm\n",
    "            / 2.0\n",
    "        )\n",
    "        loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=true_box_class, logits=pred_box_class\n",
    "        )\n",
    "        loss_class = tf.reduce_sum(loss_class * class_mask) / nb_class_norm\n",
    "\n",
    "        # combine all terms\n",
    "        loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=10, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5',\n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(Sequence):\n",
    "    def __init__(self, images,\n",
    "                       config,\n",
    "                       shuffle=True,\n",
    "                       jitter=True,\n",
    "                       norm=None):\n",
    "        self.generator = None\n",
    "\n",
    "        self.images = images\n",
    "        self.config = config\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.jitter  = jitter\n",
    "        self.norm    = norm\n",
    "\n",
    "        self.counter = 0\n",
    "        self.anchors = [BoundBox(0, 0, config['ANCHORS'][2*i], config['ANCHORS'][2*i+1]) for i in range(int(len(config['ANCHORS'])//2))]\n",
    "\n",
    "        ### augmentors by https://github.com/aleju/imgaug\n",
    "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "        # Define our sequence of augmentation steps that will be applied to every image\n",
    "        self.aug_pipe = iaa.Sequential(\n",
    "            [\n",
    "                iaa.SomeOf((0, 5),\n",
    "                    [\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "                            iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                            iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                        ]),\n",
    "                        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                        ]),\n",
    "                        iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                        iaa.Multiply((0.5, 1.5), per_channel=0.5), # change brightness of images (50-150% of original value)\n",
    "                        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "                    ],\n",
    "                    random_order=True\n",
    "                )\n",
    "            ],\n",
    "            random_order=True\n",
    "        )\n",
    "\n",
    "        if shuffle: np.random.shuffle(self.images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(float(len(self.images))/self.config['BATCH_SIZE']))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        l_bound = idx*self.config['BATCH_SIZE']\n",
    "        r_bound = (idx+1)*self.config['BATCH_SIZE']\n",
    "\n",
    "        if r_bound > len(self.images):\n",
    "            r_bound = len(self.images)\n",
    "            l_bound = r_bound - self.config['BATCH_SIZE']\n",
    "\n",
    "        instance_count = 0\n",
    "\n",
    "        x_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))\n",
    "        b_batch = np.zeros((r_bound - l_bound, 1     , 1     , 1    ,  self.config['TRUE_BOX_BUFFER'], 4))\n",
    "        y_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'],  self.config['GRID_W'], self.config['BOX'], 4+1+self.config['CLASS']))\n",
    "\n",
    "        for train_instance in self.images[l_bound:r_bound]:\n",
    "            # augment input image and fix object's position and size\n",
    "            img, all_objs = self.aug_image(train_instance, jitter=self.jitter)\n",
    "\n",
    "            # construct output from object's x, y, w, h\n",
    "            true_box_index = 0\n",
    "\n",
    "            for obj in all_objs:\n",
    "                if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin'] and obj['name'] in self.config['LABELS']:\n",
    "                    center_x = .5*(obj['xmin'] + obj['xmax'])\n",
    "                    center_x = center_x / (float(self.config['IMAGE_W']) / self.config['GRID_W'])\n",
    "                    center_y = .5*(obj['ymin'] + obj['ymax'])\n",
    "                    center_y = center_y / (float(self.config['IMAGE_H']) / self.config['GRID_H'])\n",
    "\n",
    "                    grid_x = int(np.floor(center_x))\n",
    "                    grid_y = int(np.floor(center_y))\n",
    "\n",
    "                    if grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
    "                        obj_indx  = self.config['LABELS'].index(obj['name'])\n",
    "\n",
    "                        center_w = (obj['xmax'] - obj['xmin']) / (float(self.config['IMAGE_W']) / self.config['GRID_W']) # unit: grid cell\n",
    "                        center_h = (obj['ymax'] - obj['ymin']) / (float(self.config['IMAGE_H']) / self.config['GRID_H']) # unit: grid cell\n",
    "\n",
    "                        box = [center_x, center_y, center_w, center_h]\n",
    "\n",
    "                        # find the anchor that best predicts this box\n",
    "                        best_anchor = -1\n",
    "                        max_iou     = -1\n",
    "\n",
    "                        shifted_box = BoundBox(0,\n",
    "                                               0,\n",
    "                                               center_w,\n",
    "                                               center_h)\n",
    "\n",
    "                        for i in range(len(self.anchors)):\n",
    "                            anchor = self.anchors[i]\n",
    "                            iou    = bbox_iou(shifted_box, anchor)\n",
    "\n",
    "                            if max_iou < iou:\n",
    "                                best_anchor = i\n",
    "                                max_iou     = iou\n",
    "\n",
    "                        # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 4  ] = 1.\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 5+obj_indx] = 1\n",
    "\n",
    "                        # assign the true box to b_batch\n",
    "                        b_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
    "\n",
    "                        true_box_index += 1\n",
    "                        true_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
    "\n",
    "            # assign input image to x_batch\n",
    "            if self.norm != None:\n",
    "                x_batch[instance_count] = self.norm(img)\n",
    "            else:\n",
    "                # plot image and bounding boxes for sanity check\n",
    "                for obj in all_objs:\n",
    "                    if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin']:\n",
    "                        cv2.rectangle(img[:,:,::-1], (obj['xmin'],obj['ymin']), (obj['xmax'],obj['ymax']), (255,0,0), 3)\n",
    "                        cv2.putText(img[:,:,::-1], obj['name'],\n",
    "                                    (obj['xmin']+2, obj['ymin']+12),\n",
    "                                    0, 1.2e-3 * img.shape[0],\n",
    "                                    (0,255,0), 2)\n",
    "\n",
    "                x_batch[instance_count] = img\n",
    "\n",
    "            # increase instance counter in current batch\n",
    "            instance_count += 1\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "        return [x_batch, b_batch], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.images)\n",
    "        self.counter = 0\n",
    "\n",
    "    def aug_image(self, train_instance, jitter):\n",
    "        image_name = train_instance['file_name']\n",
    "        image = cv2.imread(image_name)\n",
    "        \n",
    "        h, w, c = image.shape\n",
    "\n",
    "        all_objs = copy.deepcopy(train_instance['object'])\n",
    "\n",
    "        if jitter:\n",
    "            ### scale the image\n",
    "            scale = np.random.uniform() / 10. + 1.\n",
    "            image = cv2.resize(image, (0,0), fx = scale, fy = scale)\n",
    "\n",
    "            ### translate the image\n",
    "            max_offx = (scale-1.) * w\n",
    "            max_offy = (scale-1.) * h\n",
    "            offx = int(np.random.uniform() * max_offx)\n",
    "            offy = int(np.random.uniform() * max_offy)\n",
    "\n",
    "            image = image[offy : (offy + h), offx : (offx + w)]\n",
    "\n",
    "            ### flip the image\n",
    "            flip = np.random.binomial(1, .5)\n",
    "            if flip > 0.5: image = cv2.flip(image, 1)\n",
    "\n",
    "            #image = self.aug_pipe.augment_image(image)\n",
    "\n",
    "        # resize the image to standard size\n",
    "        image = cv2.resize(image, (self.config['IMAGE_H'], self.config['IMAGE_W']))\n",
    "        image = image[:,:,::-1]\n",
    "\n",
    "        # fix object's position and size\n",
    "        for obj in all_objs:\n",
    "            for attr in ['xmin', 'xmax']:\n",
    "                if jitter: obj[attr] = int(obj[attr] * scale - offx)\n",
    "\n",
    "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_W']) / w)\n",
    "                obj[attr] = max(min(obj[attr], self.config['IMAGE_W']), 0)\n",
    "\n",
    "            for attr in ['ymin', 'ymax']:\n",
    "                if jitter: obj[attr] = int(obj[attr] * scale - offy)\n",
    "\n",
    "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_H']) / h)\n",
    "                obj[attr] = max(min(obj[attr], self.config['IMAGE_H']), 0)\n",
    "\n",
    "            if jitter and flip > 0.5:\n",
    "                xmin = obj['xmin']\n",
    "                obj['xmin'] = self.config['IMAGE_W'] - obj['xmax']\n",
    "                obj['xmax'] = self.config['IMAGE_W'] - xmin\n",
    "\n",
    "        return image, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n"
     ]
    }
   ],
   "source": [
    "training_data_percentage = 0.95\n",
    "train_valid_split = int(training_data_percentage * len(all_imgs))\n",
    "\n",
    "train_batch = BatchGenerator(all_imgs[:train_valid_split], generator_config, norm=utils.normalize)\n",
    "valid_batch = BatchGenerator(all_imgs[train_valid_split:], generator_config, norm=utils.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 45801it [00:08, 5320.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/HGmdukdYpnyt2NV/download'\n",
    "file_name = \"pretrained_yolo_weights.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./pretrained_yolo_weights.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from disk\n"
     ]
    }
   ],
   "source": [
    "# path to file of Pascal VOC YOLO weights\n",
    "wt_path = os.path.join(os.getcwd(), './pretrained_yolo_weights.h5')\n",
    "model.load_weights(wt_path)\n",
    "print(\"Weights loaded from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to best model\n",
    "#from tensorflow import keras\n",
    "#model = keras.models.load_model('model')\n",
    "#print(\"Weights loaded from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  2/132 [..............................] - ETA: 38s - loss: 1.2117WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2067s vs `on_train_batch_end` time: 0.3866s). Check your callbacks.\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.9926\n",
      "Epoch 00001: val_loss improved from inf to 0.99074, saving model to best_model.h5\n",
      "132/132 [==============================] - 84s 633ms/step - loss: 0.9926 - val_loss: 0.9907\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.7018\n",
      "Epoch 00002: val_loss improved from 0.99074 to 0.69631, saving model to best_model.h5\n",
      "132/132 [==============================] - 88s 665ms/step - loss: 0.7018 - val_loss: 0.6963\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5266\n",
      "Epoch 00003: val_loss improved from 0.69631 to 0.57184, saving model to best_model.h5\n",
      "132/132 [==============================] - 89s 675ms/step - loss: 0.5266 - val_loss: 0.5718\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.4294\n",
      "Epoch 00004: val_loss improved from 0.57184 to 0.55961, saving model to best_model.h5\n",
      "132/132 [==============================] - 90s 682ms/step - loss: 0.4294 - val_loss: 0.5596\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.3415\n",
      "Epoch 00005: val_loss improved from 0.55961 to 0.48749, saving model to best_model.h5\n",
      "132/132 [==============================] - 90s 682ms/step - loss: 0.3415 - val_loss: 0.4875\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2857\n",
      "Epoch 00006: val_loss did not improve from 0.48749\n",
      "132/132 [==============================] - 84s 640ms/step - loss: 0.2857 - val_loss: 0.5709\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.2288\n",
      "Epoch 00007: val_loss did not improve from 0.48749\n",
      "132/132 [==============================] - 84s 640ms/step - loss: 0.2288 - val_loss: 0.5821\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1836\n",
      "Epoch 00008: val_loss did not improve from 0.48749\n",
      "132/132 [==============================] - 84s 638ms/step - loss: 0.1836 - val_loss: 0.6376\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1651\n",
      "Epoch 00009: val_loss did not improve from 0.48749\n",
      "132/132 [==============================] - 84s 639ms/step - loss: 0.1651 - val_loss: 0.7221\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1404\n",
      "Epoch 00010: val_loss did not improve from 0.48749\n",
      "132/132 [==============================] - 84s 640ms/step - loss: 0.1404 - val_loss: 0.7191\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "learning_rate = 1e-5\n",
    "\n",
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "loss = Loss(BATCH_SIZE, GRID_H, GRID_W, ANCHORS, n_boxes = 5)\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_batch,\n",
    "            validation_data = valid_batch,\n",
    "            epochs=n_epoch,\n",
    "            callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_image(image, slice_size):\n",
    "    slices = []\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "    slices_y = math.ceil(h / slice_size)\n",
    "    slices_x = math.ceil(w / slice_size)\n",
    "    \n",
    "    for y in range(slices_y):\n",
    "        for x in range(slices_x):\n",
    "\n",
    "            x_min = x * slice_size\n",
    "            y_min = y * slice_size\n",
    "\n",
    "            if x_min + slice_size > w: x_min = w - slice_size\n",
    "            if y_min + slice_size > h: y_min = h - slice_size\n",
    "\n",
    "            slices.append((x_min, y_min))\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the best threshold for all (or a part of) images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join('./', 'data', 'images/')\n",
    "#num_list = range(1, 151)\n",
    "num_list = random.sample(range(1, 151), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "thresholds = np.arange(0.1, 0.4, 0.003)\n",
    "slice_size = 256\n",
    "\n",
    "for im_n in num_list:\n",
    "\n",
    "    w = 2000\n",
    "    h = 2000\n",
    "    image = cv2.imread(image_path + str(im_n).zfill(3) + '.tiff')\n",
    "    #image = image[0:h, 0:w]\n",
    "    \n",
    "    true_data = df.loc[df['image_id'] == im_n]['box'].tolist()\n",
    "    boxes = []\n",
    "    \n",
    "    obj_threshold=thresholds[0]\n",
    "    slices = slice_image(image, slice_size)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    for slice_xy in slices:\n",
    "        image_slice = image[slice_xy[1]:slice_xy[1]+slice_size, slice_xy[0]:slice_xy[0]+slice_size]\n",
    "        slice_boxes = predict_bounding_box(image_slice, model, obj_threshold, NMS_THRESHOLD, ANCHORS, CLASS)\n",
    "        slice_boxes = [box for box in slice_boxes if box.label == 1]\n",
    "        for slice_box in slice_boxes:\n",
    "            slice_box.x = slice_size / w * slice_box.x + slice_xy[0] / w\n",
    "            slice_box.y = slice_size / h * slice_box.y + slice_xy[1] / h\n",
    "        boxes = boxes + slice_boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        box.x = w * box.x\n",
    "        box.y = h * box.y\n",
    "\n",
    "    to_remove = []\n",
    "    for i, box_1 in enumerate(boxes):\n",
    "        for box_2 in boxes:\n",
    "            if ((box_1.x < box_2.x + box_size*1.5 and box_1.x > box_2.x - box_size*1.5) and\n",
    "                (box_1.y < box_2.y + box_size*1.5 and box_1.y > box_2.y - box_size*1.5) and\n",
    "                box_1.score < box_2.score):\n",
    "                    to_remove.append(i)\n",
    "\n",
    "    for index in sorted(set(to_remove), reverse=True):\n",
    "        del boxes[index]\n",
    "    \n",
    "    f1 = []\n",
    "    for obj_threshold in thresholds:\n",
    "\n",
    "        slice_boxes = [box for box in boxes if box.score > obj_threshold]\n",
    "        positives = []\n",
    "        for i, box_1 in enumerate(slice_boxes):\n",
    "            for j, box_2 in enumerate(true_data):\n",
    "                if ((box_1.x < box_2[0] + 3/2 * box_size and box_1.x > box_2[0] - 0.5 * box_size) and\n",
    "                    (box_1.y < box_2[1] + 3/2 * box_size and box_1.y > box_2[1] - 0.5 * box_size)):\n",
    "                        positives.append(i)\n",
    "        total = len(slice_boxes)\n",
    "        if total == 0: total = -1\n",
    "        precision = len(set(positives)) / total\n",
    "        recall = len(set(positives)) / 6\n",
    "        if (precision + recall) == 0 : precision = -1\n",
    "        f1.append(2 * (precision * recall) / (precision + recall))\n",
    "    \n",
    "    f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16100000000000003"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_f1 = np.mean(f1_scores, axis=0)\n",
    "best_threshold = thresholds[np.where(avg_f1 == np.amax(avg_f1))[0][0]]\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions for test data using the best model and its best threshold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox2csv(image_n, boxes, csv_file):\n",
    "    \n",
    "    csv_out = open(csv_file, 'a')\n",
    "    \n",
    "    slide = str(image_n).zfill(2)\n",
    "    \n",
    "    for box in boxes:\n",
    "        \n",
    "        line = slide + ',' + str(round(box.x)) + ',' + str(round(box.y))\n",
    "        csv_out.write(line + '\\n')\n",
    "\n",
    "    csv_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 finished!\n",
      "10 finished!\n",
      "20 finished!\n",
      "30 finished!\n"
     ]
    }
   ],
   "source": [
    "out_dir = './output'\n",
    "#test_dir = os.path.join('./','sn_test_data/')\n",
    "test_dir = os.path.join('./','test_data/')\n",
    "file_name = 'output'\n",
    "\n",
    "slice_size = 256\n",
    "obj_threshold = best_threshold\n",
    "\n",
    "header = 'slide, x, y\\n'\n",
    "csv_file = os.path.join(out_dir,'result.csv')\n",
    "\n",
    "with open(csv_file, 'w') as csv_out:\n",
    "    csv_out.write(header)\n",
    "\n",
    "for image_n in range(1, 35):\n",
    "    \n",
    "    boxes = []\n",
    "    #image_name = file_name + str(image_n) + '.tiff'\n",
    "    image_name = str(image_n).zfill(2) + '.tif' \n",
    "    image = cv2.imread(test_dir + image_name)\n",
    "    slices = slice_image(image, slice_size)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    for slice_xy in slices:\n",
    "        image_slice = image[slice_xy[1]:slice_xy[1]+slice_size, slice_xy[0]:slice_xy[0]+slice_size]\n",
    "        slice_boxes = predict_bounding_box(image_slice, model, obj_threshold, NMS_THRESHOLD, ANCHORS, CLASS)\n",
    "        slice_boxes = [box for box in slice_boxes if box.label == 1]\n",
    "        for slice_box in slice_boxes:\n",
    "            slice_box.x = slice_size / w * slice_box.x + slice_xy[0] / w\n",
    "            slice_box.y = slice_size / h * slice_box.y + slice_xy[1] / h\n",
    "        boxes = boxes + slice_boxes\n",
    "    \n",
    "    for box in boxes:\n",
    "        box.x = w * box.x\n",
    "        box.y = h * box.y\n",
    "    \n",
    "    to_remove = []\n",
    "    for i, box_1 in enumerate(boxes):\n",
    "        for box_2 in boxes:\n",
    "            if ((box_1.x < box_2.x + box_size*1.5 and box_1.x > box_2.x - box_size*1.5) and\n",
    "                (box_1.y < box_2.y + box_size*1.5 and box_1.y > box_2.y - box_size*1.5) and\n",
    "                box_1.score < box_2.score):\n",
    "                    to_remove.append(i)\n",
    "\n",
    "    for index in sorted(set(to_remove), reverse=True):\n",
    "        del boxes[index]\n",
    "        \n",
    "    bbox2csv(image_n, boxes, csv_file)\n",
    "    if image_n in [1, 10, 20, 30]:\n",
    "        print(image_n, 'finished!')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
